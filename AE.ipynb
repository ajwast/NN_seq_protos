{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5934413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from terminaltables import AsciiTable\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89015bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collection of activation functions\n",
    "# Reference: https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "class Sigmoid():\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "\n",
    "\n",
    "class ReLU():\n",
    "    def __call__(self, x):\n",
    "        return np.where(x >= 0, x, 0)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Adam():\n",
    "    def __init__(self, learning_rate=0.001, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 1e-8\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        # Decay rates\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros(np.shape(grad_wrt_w))\n",
    "            self.v = np.zeros(np.shape(grad_wrt_w))\n",
    "        \n",
    "        self.m = self.b1 * self.m + (1 - self.b1) * grad_wrt_w\n",
    "        self.v = self.b2 * self.v + (1 - self.b2) * np.power(grad_wrt_w, 2)\n",
    "\n",
    "        m_hat = self.m / (1 - self.b1)\n",
    "        v_hat = self.v / (1 - self.b2)\n",
    "\n",
    "        self.w_updt = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "        return w - self.w_updt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c0341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "    \n",
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def acc(self, y, p):\n",
    "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - (y / p) + (1 - y) / (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14d6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        \"\"\" Sets the shape that the layer expects of the input in the forward\n",
    "        pass method \"\"\"\n",
    "        self.input_shape = shape\n",
    "\n",
    "    def layer_name(self):\n",
    "        \"\"\" The name of the layer. Used in model summary. \"\"\"\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\" The number of trainable parameters used by the layer \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def forward_pass(self, X, training):\n",
    "        \"\"\" Propogates the signal forward in the network \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        \"\"\" Propogates the accumulated gradient backwards in the network.\n",
    "        If the has trainable weights then these weights are also tuned in this method.\n",
    "        As input (accum_grad) it receives the gradient with respect to the output of the layer and\n",
    "        returns the gradient with respect to the output of the previous layer. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def output_shape(self):\n",
    "        \"\"\" The shape of the output produced by forward_pass \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "activation_functions = {\n",
    "    'relu': ReLU,\n",
    "    'sigmoid': Sigmoid\n",
    "\n",
    "}\n",
    "        \n",
    "class Activation(Layer):\n",
    "    \"\"\"A layer that applies an activation operation to the input.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name: string\n",
    "        The name of the activation function that will be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.activation_name = name\n",
    "        self.activation_func = activation_functions[name]()\n",
    "        self.trainable = True\n",
    "\n",
    "    def layer_name(self):\n",
    "        return \"Activation (%s)\" % (self.activation_func.__class__.__name__)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return self.activation_func(X)\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        return accum_grad * self.activation_func.gradient(self.layer_input)\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.input_shape\n",
    "    \n",
    "class BatchNormalization(Layer):\n",
    "    \"\"\"Batch normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.trainable = True\n",
    "        self.eps = 0.01\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        # Initialize the parameters\n",
    "        self.gamma  = np.ones(self.input_shape)\n",
    "        self.beta = np.zeros(self.input_shape)\n",
    "        # parameter optimizers\n",
    "        self.gamma_opt  = copy.copy(optimizer)\n",
    "        self.beta_opt = copy.copy(optimizer)\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.prod(self.gamma.shape) + np.prod(self.beta.shape)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "\n",
    "        # Initialize running mean and variance if first run\n",
    "        if self.running_mean is None:\n",
    "            self.running_mean = np.mean(X, axis=0)\n",
    "            self.running_var = np.var(X, axis=0)\n",
    "\n",
    "        if training and self.trainable:\n",
    "            mean = np.mean(X, axis=0)\n",
    "            var = np.var(X, axis=0)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        # Statistics saved for backward pass\n",
    "        self.X_centered = X - mean\n",
    "        self.stddev_inv = 1 / np.sqrt(var + self.eps)\n",
    "\n",
    "        X_norm = self.X_centered * self.stddev_inv\n",
    "        output = self.gamma * X_norm + self.beta\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "\n",
    "        # Save parameters used during the forward pass\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # If the layer is trainable the parameters are updated\n",
    "        if self.trainable:\n",
    "            X_norm = self.X_centered * self.stddev_inv\n",
    "            grad_gamma = np.sum(accum_grad * X_norm, axis=0)\n",
    "            grad_beta = np.sum(accum_grad, axis=0)\n",
    "\n",
    "            self.gamma = self.gamma_opt.update(self.gamma, grad_gamma)\n",
    "            self.beta = self.beta_opt.update(self.beta, grad_beta)\n",
    "\n",
    "        batch_size = accum_grad.shape[0]\n",
    "\n",
    "        # The gradient of the loss with respect to the layer inputs (use weights and statistics from forward pass)\n",
    "        accum_grad = (1 / batch_size) * gamma * self.stddev_inv * (\n",
    "            batch_size * accum_grad\n",
    "            - np.sum(accum_grad, axis=0)\n",
    "            - self.X_centered * self.stddev_inv**2 * np.sum(accum_grad * self.X_centered, axis=0)\n",
    "            )\n",
    "\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6114d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"A fully-connected NN layer.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_units: int\n",
    "        The number of neurons in the layer.\n",
    "    input_shape: tuple\n",
    "        The expected input shape of the layer. For dense layers a single digit specifying\n",
    "        the number of features of the input. Must be specified if it is the first layer in\n",
    "        the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, input_shape=None):\n",
    "        self.layer_input = None\n",
    "        self.input_shape = input_shape\n",
    "        self.n_units = n_units\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.w0 = None\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        # Initialize the weights\n",
    "        limit = 1 / math.sqrt(self.input_shape[0])\n",
    "        self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
    "        self.w0 = np.zeros((1, self.n_units))\n",
    "        # Weight optimizers\n",
    "        self.W_opt  = copy.copy(optimizer)\n",
    "        self.w0_opt = copy.copy(optimizer)\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return X.dot(self.W) + self.w0\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        # Save weights used during forwards pass\n",
    "        W = self.W\n",
    "\n",
    "        if self.trainable:\n",
    "            # Calculate gradient w.r.t layer weights\n",
    "            grad_w = self.layer_input.T.dot(accum_grad)\n",
    "            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n",
    "\n",
    "            # Update the layer weights\n",
    "            self.W = self.W_opt.update(self.W, grad_w)\n",
    "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
    "\n",
    "        # Return accumulated gradient for next layer\n",
    "        # Calculated based on the weights used during the forward pass\n",
    "        accum_grad = accum_grad.dot(W.T)\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        return (self.n_units, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd44d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(X, y=None, batch_size=64):\n",
    "    \"\"\" Simple batch generator \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    for i in np.arange(0, n_samples, batch_size):\n",
    "        begin, end = i, min(i+batch_size, n_samples)\n",
    "        if y is not None:\n",
    "            yield X[begin:end], y[begin:end]\n",
    "        else:\n",
    "            yield X[begin:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13ad65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_widgets = [\n",
    "    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
    "    ' ', progressbar.ETA()\n",
    "]\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"Neural Network. Deep Learning base model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    optimizer: class\n",
    "        The weight optimizer that will be used to tune the weights in order of minimizing\n",
    "        the loss.\n",
    "    loss: class\n",
    "        Loss function used to measure the model's performance. SquareLoss or CrossEntropy.\n",
    "    validation: tuple\n",
    "        A tuple containing validation data and labels (X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, loss, validation_data=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.layers = []\n",
    "        self.errors = {\"training\": [], \"validation\": []}\n",
    "        self.loss_function = loss()\n",
    "        self.progressbar = progressbar.ProgressBar(widgets=bar_widgets)\n",
    "\n",
    "        self.val_set = None\n",
    "        if validation_data:\n",
    "            X, y = validation_data\n",
    "            self.val_set = {\"X\": X, \"y\": y}\n",
    "\n",
    "    def set_trainable(self, trainable):\n",
    "        \"\"\" Method which enables freezing of the weights of the network's layers. \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.trainable = trainable\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\" Method which adds a layer to the neural network \"\"\"\n",
    "        # If this is not the first layer added then set the input shape\n",
    "        # to the output shape of the last added layer\n",
    "        if self.layers:\n",
    "            layer.set_input_shape(shape=self.layers[-1].output_shape())\n",
    "\n",
    "        # If the layer has weights that needs to be initialized \n",
    "        if hasattr(layer, 'initialize'):\n",
    "            layer.initialize(optimizer=self.optimizer)\n",
    "\n",
    "        # Add layer to the network\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def test_on_batch(self, X, y):\n",
    "        \"\"\" Evaluates the model over a single batch of samples \"\"\"\n",
    "        y_pred = self._forward_pass(X, training=False)\n",
    "        loss = np.mean(self.loss_function.loss(y, y_pred))\n",
    "        acc = self.loss_function.acc(y, y_pred)\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def train_on_batch(self, X, y):\n",
    "        \"\"\" Single gradient update over one batch of samples \"\"\"\n",
    "        y_pred = self._forward_pass(X)\n",
    "        loss = np.mean(self.loss_function.loss(y, y_pred))\n",
    "        acc = self.loss_function.acc(y, y_pred)\n",
    "        # Calculate the gradient of the loss function wrt y_pred\n",
    "        loss_grad = self.loss_function.gradient(y, y_pred)\n",
    "        # Backpropagate. Update weights\n",
    "        self._backward_pass(loss_grad=loss_grad)\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def fit(self, X, y, n_epochs, batch_size):\n",
    "        \"\"\" Trains the model for a fixed number of epochs \"\"\"\n",
    "        for _ in self.progressbar(range(n_epochs)):\n",
    "            \n",
    "            batch_error = []\n",
    "            for X_batch, y_batch in batch_iterator(X, y, batch_size=batch_size):\n",
    "                loss, _ = self.train_on_batch(X_batch, y_batch)\n",
    "                batch_error.append(loss)\n",
    "\n",
    "            self.errors[\"training\"].append(np.mean(batch_error))\n",
    "\n",
    "            if self.val_set is not None:\n",
    "                val_loss, _ = self.test_on_batch(self.val_set[\"X\"], self.val_set[\"y\"])\n",
    "                self.errors[\"validation\"].append(val_loss)\n",
    "\n",
    "        return self.errors[\"training\"], self.errors[\"validation\"]\n",
    "\n",
    "    def _forward_pass(self, X, training=True):\n",
    "        \"\"\" Calculate the output of the NN \"\"\"\n",
    "        layer_output = X\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer.forward_pass(layer_output, training)\n",
    "\n",
    "        return layer_output\n",
    "\n",
    "    def _backward_pass(self, loss_grad):\n",
    "        \"\"\" Propagate the gradient 'backwards' and update the weights in each layer \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward_pass(loss_grad)\n",
    "\n",
    "    def summary(self, name=\"Model Summary\"):\n",
    "        # Print model name\n",
    "        print (AsciiTable([[name]]).table)\n",
    "        # Network input shape (first layer's input shape)\n",
    "        print (\"Input Shape: %s\" % str(self.layers[0].input_shape))\n",
    "        # Iterate through network and get each layer's configuration\n",
    "        table_data = [[\"Layer Type\", \"Parameters\", \"Output Shape\"]]\n",
    "        tot_params = 0\n",
    "        for layer in self.layers:\n",
    "            layer_name = layer.layer_name()\n",
    "            params = layer.parameters()\n",
    "            out_shape = layer.output_shape()\n",
    "            table_data.append([layer_name, str(params), str(out_shape)])\n",
    "            tot_params += params\n",
    "        # Print network configuration table\n",
    "        print (AsciiTable(table_data).table)\n",
    "        print (\"Total Parameters: %d\\n\" % tot_params)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Use the trained model to predict labels of X \"\"\"\n",
    "        return self._forward_pass(X, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e66a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.input_dim = 32\n",
    "        self.latent_dim = 4 # The dimension of the data embedding\n",
    "\n",
    "        optimizer = Adam(learning_rate=0.01, b1=0.5)\n",
    "        loss_function = CrossEntropy\n",
    "\n",
    "        self.encoder = self.build_encoder(optimizer, loss_function)\n",
    "        self.decoder = self.build_decoder(optimizer, loss_function)\n",
    "\n",
    "        self.autoencoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        self.autoencoder.layers.extend(self.encoder.layers)\n",
    "        self.autoencoder.layers.extend(self.decoder.layers)\n",
    "\n",
    "        print ()\n",
    "        self.autoencoder.summary(name=\"RhythNN\")\n",
    "\n",
    "    def build_encoder(self, optimizer, loss_function):\n",
    "\n",
    "        encoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        encoder.add(Dense(16, input_shape=(self.input_dim,)))\n",
    "        encoder.add(Activation('relu'))\n",
    "        encoder.add(BatchNormalization(momentum=0.8))\n",
    "        encoder.add(Dense(8))\n",
    "        encoder.add(Activation('relu'))\n",
    "        encoder.add(BatchNormalization(momentum=0.8))\n",
    "        encoder.add(Dense(self.latent_dim))\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def build_decoder(self, optimizer, loss_function):\n",
    "\n",
    "        decoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        decoder.add(Dense(8, input_shape=(self.latent_dim,)))\n",
    "        decoder.add(Activation('relu'))\n",
    "        decoder.add(BatchNormalization(momentum=0.8))\n",
    "        decoder.add(Dense(16))\n",
    "        decoder.add(Activation('relu'))\n",
    "        decoder.add(BatchNormalization(momentum=0.8))\n",
    "        decoder.add(Dense(self.input_dim))\n",
    "        decoder.add(Activation('sigmoid'))\n",
    "\n",
    "        return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39814eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c983194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---------+\n",
      "| RhythNN |\n",
      "+---------+\n",
      "Input Shape: (32,)\n",
      "+----------------------+------------+--------------+\n",
      "| Layer Type           | Parameters | Output Shape |\n",
      "+----------------------+------------+--------------+\n",
      "| Dense                | 528        | (16,)        |\n",
      "| Activation (ReLU)    | 0          | (16,)        |\n",
      "| BatchNormalization   | 32         | (16,)        |\n",
      "| Dense                | 136        | (8,)         |\n",
      "| Activation (ReLU)    | 0          | (8,)         |\n",
      "| BatchNormalization   | 16         | (8,)         |\n",
      "| Dense                | 36         | (4,)         |\n",
      "| Dense                | 40         | (8,)         |\n",
      "| Activation (ReLU)    | 0          | (8,)         |\n",
      "| BatchNormalization   | 16         | (8,)         |\n",
      "| Dense                | 144        | (16,)        |\n",
      "| Activation (ReLU)    | 0          | (16,)        |\n",
      "| BatchNormalization   | 32         | (16,)        |\n",
      "| Dense                | 544        | (32,)        |\n",
      "| Activation (Sigmoid) | 0          | (32,)        |\n",
      "+----------------------+------------+--------------+\n",
      "Total Parameters: 1524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ae = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ba315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% [-------------------------------------------------] Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6738063440855796,\n",
       "  0.5909010830439506,\n",
       "  0.5216445657314778,\n",
       "  0.4578236664404687,\n",
       "  0.40595069303146514,\n",
       "  0.36466669253560247,\n",
       "  0.3332987622537195,\n",
       "  0.30990917239423066,\n",
       "  0.29097820605010244,\n",
       "  0.27702613508601626,\n",
       "  0.2649450021534426,\n",
       "  0.2561516928174888,\n",
       "  0.2484839673519807,\n",
       "  0.24248659583113116,\n",
       "  0.23800250551144783,\n",
       "  0.23266924112344176,\n",
       "  0.22752105644161694,\n",
       "  0.2235107886931343,\n",
       "  0.219955544052418,\n",
       "  0.21742440011757846,\n",
       "  0.21386852456840116,\n",
       "  0.2113346183930884,\n",
       "  0.20904211100820974,\n",
       "  0.20666251451664558,\n",
       "  0.20412949600954927,\n",
       "  0.2019157548714623,\n",
       "  0.19974569291850583,\n",
       "  0.1976158249267004,\n",
       "  0.19562125200862646,\n",
       "  0.19402265980909036],\n",
       " [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.autoencoder.fit(dataset,dataset, 30, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d971509",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae2 = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae2.autoencoder.fit(dataset,dataset, 20, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonosc import osc_message_builder\n",
    "from pythonosc import osc_bundle_builder\n",
    "from pythonosc import udp_client\n",
    "\n",
    "# Set up OSC client\n",
    "ip_address = '127.0.0.1'  # Change this to the IP address of your OSC receiver\n",
    "port = 9000  # Change this to the port number of your OSC receiver\n",
    "client = udp_client.SimpleUDPClient(ip_address, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_vector = np.array([[1, -4, -2, 5]])\n",
    "\n",
    "generated_rhythm = ae.decoder.predict(latent_vector)\n",
    "\n",
    "print(generated_rhythm)\n",
    "\n",
    "output = []\n",
    "tolerance=0.5\n",
    "for op in generated_rhythm[0]:\n",
    "    if op > tolerance:\n",
    "        output.append(1)\n",
    "    else: output.append(0)\n",
    "\n",
    "\n",
    "print(output)\n",
    "# client.send_message('/generator', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aabf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[12]\n",
    "reconstruction = ae.autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pythonosc.dispatcher import Dispatcher\n",
    "from pythonosc.osc_server import BlockingOSCUDPServer\n",
    "import threading\n",
    "from typing import List, Any\n",
    "\n",
    "x_value1 = 0\n",
    "y_value1 = 0\n",
    "x_value2 = 0\n",
    "y_value2 = 0\n",
    "x_value3 = 0\n",
    "y_value3 = 0\n",
    "x_value4 = 0\n",
    "y_value4 = 0\n",
    "t1 = 0.5\n",
    "t2 = 0.5\n",
    "t3 = 0.5\n",
    "min_output = 0\n",
    "max_output = 127\n",
    "\n",
    "# Define the function that will handle the OSC message\n",
    "def handle_value1(address, *args: List[Any]):\n",
    "    #MODEL1\n",
    "    x_value1 = args[1]\n",
    "    y_value1 = args[2]\n",
    "    x_value2 = args[4]\n",
    "    y_value2 = args[5]\n",
    "    latent_vector = np.array([[x_value1, y_value1,x_value2, y_value2]]) \n",
    "    generated_rhythm1 = ae.decoder.predict(latent_vector)\n",
    "    tolerance1 = args[3]\n",
    "    output1 = []\n",
    "    vel1 = []\n",
    "    for v in generated_rhythm1[0]:\n",
    "        scaled_value = ((max_output - min_output) * v) + min_output\n",
    "        scaled_value = round(scaled_value)\n",
    "        vel1.append(scaled_value)\n",
    "\n",
    "    for op1 in generated_rhythm1[0]:\n",
    "        #raw.append(op)\n",
    "        if op1 > tolerance1:\n",
    "            output1.append(1)\n",
    "        else: output1.append(0)\n",
    "\n",
    "    client.send_message('/generator', output1)\n",
    "    client.send_message('/vel1', vel1)   \n",
    "    \n",
    "def handle_value2(address, *args: List[Any]):\n",
    "    #MODEL2\n",
    "    x_value3 = args[1]\n",
    "    y_value3 = args[2]\n",
    "    x_value4 = args[4]\n",
    "    y_value4 = args[5]\n",
    "    latent_vector2= np.array([[x_value3, y_value3,x_value4, y_value4]])\n",
    "    generated_rhythm2 = ae2.decoder.predict(latent_vector2)\n",
    "    tolerance2 = args[3]\n",
    "    output2 = []\n",
    "    vel2 = []\n",
    "    for v2 in generated_rhythm2[0]:\n",
    "        scaled_value = ((max_output - min_output) * v2) + min_output\n",
    "        scaled_value = round(scaled_value)\n",
    "        vel2.append(scaled_value)\n",
    "\n",
    "    for op2 in generated_rhythm2[0]:\n",
    "        if op2 > tolerance2:\n",
    "            output2.append(1)\n",
    "        else: output2.append(0)\n",
    "    client.send_message('/generator2', output2)\n",
    "    client.send_message('/vel2', vel2)\n",
    "    \n",
    "    \n",
    "# def handle_value3(address, *args):\n",
    "#     #MODEL3\n",
    "#     x_value3 = args[1]\n",
    "#     y_value3 = args[2]\n",
    "#     latent_vector = np.array([[x_value3, y_value3]]) \n",
    "#     generated_rhythm3 = generator3.predict(latent_vector)\n",
    "#     tolerance3 = args[3]\n",
    "#     output3 = []\n",
    "#     vel3 = []\n",
    "#     for v3 in generated_rhythm3[0]:\n",
    "#         scaled_value = ((max_output - min_output) * v3) + min_output\n",
    "#         scaled_value = round(scaled_value)\n",
    "#         vel3.append(scaled_value)\n",
    "\n",
    "#     for op3 in generated_rhythm3[0]:\n",
    "#         if op3 > tolerance3:\n",
    "#             output3.append(1)\n",
    "#         else: output3.append(0)\n",
    "#     #print(output)\n",
    "#     client.send_message('/generator3', output3)\n",
    "#     client.send_message('/vel3', vel3)\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "# Set up the OSC dispatcher and server\n",
    "dispatcher = Dispatcher()\n",
    "dispatcher.map(\"/gen1\", handle_value1)\n",
    "dispatcher.map(\"/gen2\", handle_value2)\n",
    "# dispatcher.map(\"/gen3\", handle_value3)\n",
    "\n",
    "server = BlockingOSCUDPServer((\"localhost\", 9001), dispatcher)\n",
    "\n",
    "# Start the OSC server in a separate thread\n",
    "server_thread = threading.Thread(target=server.serve_forever)\n",
    "server_thread.start()\n",
    "print('Serving')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fcf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.server_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d86e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
